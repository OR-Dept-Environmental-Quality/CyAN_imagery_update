---
title: "Prepare data for the HABs weekly report"
output: html_notebook
editor_options:
  chunk_output_type: console
---

## Import data
```{r initial}
library(dplyr)
library(reticulate)
Sys.setenv(RETICULATE_PYTHON = "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\python.exe")
Sys.setenv(PROJ_LIB = "C:\\Users\\ygrund\\AppData\\Local\\R\\win-library\\4.3\\PROJ\\proj")
# Sys.setlocale(locale = "English_United States.1252")
# Sys.setlocale("LC_ALL", "en_US.UTF-8")
data_path <- "//deqhq1/WQ-Share/Harmful Algal Blooms Coordination Team/CyAN_Weekly_Report/Data/"
data_ci <-  readxl::read_excel(paste0(data_path,"cyan_ci_2024.xlsx"))
data_dn <-  readxl::read_excel(paste0(data_path,"cyan_dn_2024.xlsx")) %>%
  dplyr::mutate_at(dplyr::vars(dplyr::starts_with("VALUE_")), as.numeric)
dates <- readxl::read_excel(paste0(data_path,"calendar-dates.xlsx"))
resolvable_lakes <- readxl::read_excel(paste0(data_path,"Resolvable_Lakes.xlsx"))

```

## Step 1. Download data from NASA's data site.

```{r OceanData}
# # Download data from OceanData site - not working
# day_start <- "2024081"
# 
# library(httr)
# 
# # Function to create a custom session for authentication
# session <- function(username, password) {
#   httr::authenticate(username, password)
# }
# 
# # Define username and password
# username <- "ygrund"
# password <- "XXX" (Not-disclosed)
# 
# # Create the session
# my_session <- session(username, password)
# 
# dates <- readxl::read_excel("./data/calendar-dates.xlsx")
# day_end <- dates %>% dplyr::filter(as.POSIXlt(Date) == as.POSIXlt(Sys.Date()-1)) %>% dplyr::pull(CyAN_File_NUM)
# hab_days <- as.character(seq(day_start, day_end))
# hab_days <- substr(hab_days, nchar(hab_days) - 2, nchar(hab_days))
# hab_days_length <- length(hab_days)
# print(paste0("Day State: ",dates[which(dates$CyAN_File_NUM == day_start),]$Date, " | ",day_start))
# print(paste0("Day End: ",dates[which(dates$CyAN_File_NUM == day_end),]$Date, " | ",day_end))
# print(paste0("Total Days | ",hab_days_length))
# 
# file_num <- dates %>% dplyr::filter(as.numeric(CyAN_File_NUM) >= as.numeric(day_start) &
#                                       as.numeric(CyAN_File_NUM) <= as.numeric(day_end))
# tiles <- c("1_1","1_2","2_1","2_2")
# 
# outPath_dn <- "//deqhq1/WQ-Share/Harmful Algal Blooms Coordination Team/Satellite data/CyAN_Data_V5/Sentinel-3/CI_cyano/raw/"
# 
# for(fileNum in file_num$CyAN_File_NUM) {
# 
#   # test: fileNum = "2024081"
#   year <- file_num %>% dplyr::filter(CyAN_File_NUM == fileNum) %>% dplyr::pull(Year)
#   filename_list <- list()
# 
#   folder_path <- paste0(outPath_dn, year, "/temp/")
#   if(!file.exists(folder_path)) {dir.create(folder_path, showWarnings = TRUE, recursive = FALSE)}else{}
# 
#   for(tile in tiles){
# 
#     # test: tile = "1_1"
#     url <- paste0("https://oceandata.sci.gsfc.nasa.gov/getfile/L",fileNum,".L3m_DAY_CYAN_CI_cyano_CYAN_CONUS_300m_",tile,".tif")
#     filename <- sub(".*/", "", url)
# 
#     tryCatch({
#       # Submit the request using the configured session
#       response <- httr::GET(
#         url,
#         httr::progress(),
#         # httr::add_headers(`User-Agent` = "Mozilla/5.0"),
#         httr::authenticate(username, password)
#       )
# 
#       # Check for successful response
#       if (httr::status_code(response) == 200) {
#         # Save the file
#         content <- httr::content(response, as = "raw")
#         writeBin(content, paste0(outPath_dn,year,"/temp/",filename))
#         print(paste0("File downloaded successfully: ",fileNum))
#       } else {
#         stop(paste0("Failed to download the file: ",filename))
#       }
#     }, error = function(e) {
#       # Handle any errors here
#       print(paste("Error:", e$message, " ", filename))
#     })
#     filename_list <- c(filename_list,filename)
#   }
# 
#   # combine four tif flies into a single file
#   raster1 <- raster::raster(paste0(outPath_dn,"/",year,"/temp/",filename_list[1]))
#   raster2 <- raster::raster(paste0(outPath_dn,"/",year,"/temp/",filename_list[2]))
#   raster3 <- raster::raster(paste0(outPath_dn,"/",year,"/temp/",filename_list[3]))
#   raster4 <- raster::raster(paste0(outPath_dn,"/",year,"/temp/",filename_list[4]))
#   raster_list <- list(raster1, raster2, raster3, raster4)
#   merged_raster <- raster::merge(raster1, raster2, raster3, raster4)
#   raster::writeRaster(merged_raster,
#                       filename = paste0(outPath_dn,year,"/",stringr::str_sub(filename, start = 1, end = 8)),
#                       format = "GTiff", overwrite = TRUE)
# }

```

```{r OceanColor}
# Download rasters from the OceanColor site
baseurl <- "https://oceancolor.gsfc.nasa.gov/CYAN/OLCI/"
outPath_dn <- "//deqhq1/WQ-Share/Harmful Algal Blooms Coordination Team/Satellite data/CyAN_Data_V5/Sentinel-3/CI_cyano/raw/"

tiles <- paste0("_", paste(c("1_1","1_2","2_1","2_2"), collapse = "|"), "\\.tif$")

day_start <- paste0(max(data_ci$Year), sprintf("%03d", max(as.numeric(data_ci$Julian_day))+1))
day_end <- dates %>% dplyr::filter(as.POSIXlt(Date) == as.POSIXlt(Sys.Date()-1)) %>% dplyr::pull(CyAN_File_NUM)
year <- substr(day_start, 1, 4)
hab_days <- as.character(seq(day_start, day_end))
hab_days <- substr(hab_days, nchar(hab_days) - 2, nchar(hab_days))
hab_days_length <- length(hab_days)
print(paste0("Day State: ",dates[which(dates$CyAN_File_NUM == day_start),]$Date, " | ",day_start))
print(paste0("Day End: ",dates[which(dates$CyAN_File_NUM == day_end),]$Date, " | ",day_end))
print(paste0("Total Days | ",hab_days_length))

download_tgz_path <- paste0(outPath_dn, year, "/temp/tgz/")
download_tgz <- paste0('L', year, hab_days, '.L3m_DAY_CYAN_CI_cyano_CYAN_CONUS_300m.tgz')
url <- paste0(baseurl, year, '/', hab_days, '/', download_tgz)

download_file_list <- list()
retry <- FALSE
for (i in 1:hab_days_length) {
  
  # test: i=1
  retry <- FALSE
  cat(url[i], "\n")
  
  tryCatch(
    {
      utils::download.file(url[i], destfile = paste0(download_tgz_path, download_tgz[i]))
      utils::untar(paste0(download_tgz_path, download_tgz[i]), exdir = download_tgz_path)
      print("Tiles download completed")
    },
    error = function(e) {
      cat("Error:", conditionMessage(e), "\n")
      retry <<- TRUE
    }
  )
  
  if(retry){next}
  
  download_folder <- sub("\\.tgz$", "", file.path(paste0(download_tgz_path, download_tgz[i])))
  downloaded_files <- list.files(download_folder, full.names = TRUE)
  tile_files_from <- grep(tiles, downloaded_files, value = TRUE)
  tile_file_names <- sub(".*/", "", tile_files_from)
  for (file in tile_files_from) {
    tile_files_to <- gsub(paste0("/tgz/",substr(download_tgz[i],1, nchar(download_tgz[i]) - 4)), "", file)
    file.copy(from = file, to = tile_files_to)
  }
  
  print("Combine four tiles into a single file")
  raster1 <- raster::raster(paste0(outPath_dn,"/",year,"/temp/",tile_file_names[1]))
  raster2 <- raster::raster(paste0(outPath_dn,"/",year,"/temp/",tile_file_names[2]))
  raster3 <- raster::raster(paste0(outPath_dn,"/",year,"/temp/",tile_file_names[3]))
  raster4 <- raster::raster(paste0(outPath_dn,"/",year,"/temp/",tile_file_names[4]))
  raster_list <- list(raster1, raster2, raster3, raster4)
  merged_raster <- raster::merge(raster1, raster2, raster3, raster4)
  raster::writeRaster(merged_raster,
                      filename = paste0(outPath_dn,year,"/",stringr::str_sub(tile_file_names[1], start = 2, end = 8)),
                      format = "GTiff", overwrite = TRUE)
  
  download_file_list <- c(download_file_list, stringr::str_sub(tile_file_names[1], start = 2, end = 8))
}

print("Step 1 completed")

```

## Step 2. Process data.
```{python python}
import sys
import arcpy
from arcpy.sa import *
import pandas as pd
import os
import shutil
import glob
import tempfile

arcpy.env.overwriteOutput = True
arcpy.CheckOutExtension("Spatial")

zones = r"\\deqhq1\WQ-Share\Harmful Algal Blooms Coordination Team\CyAN_Weekly_Report\Data\gis\CyAN_Waterbodies.shp"
base_path = r"\\deqhq1\WQ-Share\Harmful Algal Blooms Coordination Team\Satellite data\CyAN_Data_V5\Sentinel-3\CI_cyano"
habs_project_path = r"\\deqhq1\WQ-Share\Harmful Algal Blooms Coordination Team\HAB_Shiny_app\data"
# cyan_project_path = r"\\deqhq1\WQ-Share\Harmful Algal Blooms Coordination Team\CyAN_Weekly_Report\CyAN_imagery_update\data"
cyan_project_path = r"C:\Users\ygrund\ygrund_C_Users\PROJECTS\HABs\CyAN_imagery_update\data"

print("Create a temporary location to store the temporary output tables.")
temp_dir = os.path.join(base_path, "temp")
os.makedirs(temp_dir, exist_ok=True)

print("Initialize empty dataframes to store tabulate and zonal results.")
tabulate_result_df = pd.DataFrame(dtype='object')
zonal_result_df = pd.DataFrame(dtype='object')

extent = arcpy.Extent(-2315637.32, 2919264.13, -1566307.49, 2284415.24)

year = r.year
raw_path = os.path.join(base_path, f"raw\\{year}")
dn_path = os.path.join(base_path, f"dn\\{year}")
ci_path = os.path.join(base_path, f"ci\\{year}")

temp_tabulate = os.path.join(temp_dir, f"tabulate_result_{year}.dbf")
temp_zonal = os.path.join(temp_dir, f"zonal_result_{year}.dbf")

# r.download_file_list = ['2024106', '2024107']
for raw_name in r.download_file_list:
  arcpy.CheckOutExtension("Spatial")
  # test: raw_name = '2024108'
  print(raw_name)
  
  raw_tif_path = os.path.join(raw_path, f"{raw_name}.tif")
  output_dn_path = os.path.join(dn_path, f"{raw_name}.tif")
  output_ci_path = os.path.join(ci_path, f"{raw_name}.tif")
  
  raw_tif = arcpy.Raster(raw_tif_path)
  print("...Clip the raster for Oregon's extent")
  raw_tif_clip = arcpy.sa.ExtractByRectangle(raw_tif, extent)
  print("...Convert raster values to integer")
  raw_tif_int = arcpy.sa.Int(raw_tif_clip)
  print("...Set 254 and 255 to null")
  outSetNull = arcpy.sa.SetNull(raw_tif_int,raw_tif_int,"Value = 254 Or Value = 255") # 255=no_data 254=land
  print("...Convert DN to CI cells/ml")
  outCellsML = (arcpy.sa.Power(10, (3.0 / 250.0 * outSetNull - 4.2))) * 100000000
  
  print("...Check if the output raster files already exists")
  if arcpy.Exists(output_dn_path):
    arcpy.management.Delete(output_dn_path)
    
  if arcpy.Exists(output_ci_path):
    arcpy.management.Delete(output_ci_path)
    
  print("...Reproject and save the output rasters")
  out_coor_system = (
    "PROJCS['WGS_1984_Web_Mercator_Auxiliary_Sphere',"
    "GEOGCS['GCS_WGS_1984',"
    "DATUM['D_WGS_1984',"
    "SPHEROID['WGS_1984',6378137.0,298.257223563]],"
    "PRIMEM['Greenwich',0.0],"
    "UNIT['Degree',0.0174532925199433]],"
    "PROJECTION['Mercator_Auxiliary_Sphere'],"
    "PARAMETER['False_Easting',0.0],"
    "PARAMETER['False_Northing',0.0],"
    "PARAMETER['Central_Meridian',0.0],"
    "PARAMETER['Standard_Parallel_1',0.0],"
    "PARAMETER['Auxiliary_Sphere_Type',0.0],"
    "UNIT['Meter',1.0]]"
    )
    
  in_coor_system = (
    "PROJCS['USA_Contiguous_Albers_Equal_Area_Conic_USGS_version',"
    "GEOGCS['GCS_North_American_1983',"
    "DATUM['D_North_American_1983',"
    "SPHEROID['GRS_1980',6378137.0,298.257222101]],"
    "PRIMEM['Greenwich',0.0],"
    "UNIT['Degree',0.0174532925199433]],"
    "PROJECTION['Albers'],"
    "PARAMETER['False_Easting',0.0],"
    "PARAMETER['False_Northing',0.0],"
    "PARAMETER['Central_Meridian',-96.0],"
    "PARAMETER['Standard_Parallel_1',29.5],"
    "PARAMETER['Standard_Parallel_2',45.5],"
    "PARAMETER['Latitude_Of_Origin',23.0],"
    "UNIT['Meter',1.0]]"
    )
    
  arcpy.management.ProjectRaster(
    in_raster=outSetNull,
    out_raster=output_dn_path,
    out_coor_system=out_coor_system,
    resampling_type="NEAREST",
    cell_size="300 300",
    geographic_transform="WGS_1984_(ITRF00)_To_NAD_1983",
    Registration_Point="",
    in_coor_system=in_coor_system
    )
      
  arcpy.management.ProjectRaster(
    in_raster=outCellsML,
    out_raster=output_ci_path,
    out_coor_system=out_coor_system,
    resampling_type="NEAREST",
    cell_size="300 300",
    geographic_transform="WGS_1984_(ITRF00)_To_NAD_1983",
    Registration_Point="",
    in_coor_system=in_coor_system
    )
    
  print("...Copy the CI raster to the HABs project folder")
  pattern = os.path.join(ci_path, raw_name + ".*")
  files_to_copy = glob.glob(pattern)
  for file_path in files_to_copy:
    file_name = os.path.basename(file_path)
    base_name, extension = file_name.rsplit('.', 1)
    copy_ci_path_1 = os.path.join(habs_project_path, str(year), str(file_name))
    shutil.copy(file_path, copy_ci_path_1)
    
  print("...Copy the CI raster to the CyAN project folder")
  file_to_copy = os.path.join(ci_path, raw_name + ".tif")
  copy_ci_path_2 = os.path.join(cyan_project_path, str(year), raw_name + ".tif")
  shutil.copy(file_to_copy, copy_ci_path_2)
      
  print("...Tabulate Area")
  tabulate_result = arcpy.sa.TabulateArea(
    in_zone_data=zones,
    zone_field="GNISIDNAME",
    in_class_data=raw_tif_int, # Use raw_tif_int to include 254 & 255
    class_field="Value",
    out_table=temp_tabulate
    )
      
  print("...Add File_NUM")
  if "File_NUM" not in [f.name for f in arcpy.ListFields(temp_tabulate)]:
    arcpy.AddField_management(temp_tabulate, "File_NUM", "TEXT")
    
  with arcpy.da.UpdateCursor(temp_tabulate, ["File_NUM"]) as cursor:
    for row in cursor:
      row[0] = raw_name
      cursor.updateRow(row)
      
  print("...Convert tabulate result to dataframe and append to the final dataframe")
  tabulate_df = pd.DataFrame(arcpy.da.TableToNumPyArray(tabulate_result, '*'))
  if "OID" in tabulate_df.columns:
    tabulate_df = tabulate_df.drop(columns=['OID'])
  tabulate_result_df = pd.concat([tabulate_result_df, tabulate_df], ignore_index=True)
    
  print("...Zonal Statistics")
  arcpy.sa.ZonalStatisticsAsTable(
    in_zone_data=zones,
    zone_field="GNISIDNAME",
    in_value_raster=outCellsML,
    out_table=temp_zonal,
    ignore_nodata="DATA",
    statistics_type="ALL",
    process_as_multidimensional="CURRENT_SLICE",
    percentile_values=[100],
    percentile_interpolation_type="AUTO_DETECT",
    circular_calculation="ARITHMETIC",
    circular_wrap_value=360,
    out_join_layer=None
    )
      
  print("...Add File_NUM")
  if "File_NUM" not in [f.name for f in arcpy.ListFields(temp_zonal)]:
    arcpy.AddField_management(temp_zonal, "File_NUM", "TEXT")
    
  with arcpy.da.UpdateCursor(temp_zonal, ["File_NUM"]) as cursor:
    for row in cursor:
      row[0] = raw_name
      cursor.updateRow(row)
    
  print("...Convert zonal statistics result to dataframe and append to the final dataframe")
  zonal_df = pd.DataFrame(arcpy.da.TableToNumPyArray(temp_zonal, '*'))
  if "OID" in zonal_df.columns:
    zonal_df = zonal_df.drop(columns=['OID'])
  zonal_result_df = pd.concat([zonal_result_df, zonal_df], ignore_index=True)
    
  print("...Save intermediate results to CSV after each raw_name")
  tabulate_result_df.to_csv(os.path.join(temp_dir, f"tabulate_result_{raw_name}.csv"), index=False)
  zonal_result_df.to_csv(os.path.join(temp_dir, f"zonal_result_{raw_name}.csv"), index=False)
    
  print("...Clear dataframes for the next iteration")
  tabulate_result_df = pd.DataFrame(dtype='object')
  zonal_result_df = pd.DataFrame(dtype='object')

```

```{r zonal_tbl}
print("...Generate CI zonal statistics data table")
# download_file_list <- c("2024108")
# temp_dir <- "//deqhq1/WQ-Share/Harmful Algal Blooms Coordination Team/Satellite data/CyAN_Data_V5/Sentinel-3/CI_cyano/temp/"
zonal_temp_file_names <- paste0("zonal_result_", download_file_list, ".csv")
zonal_temp_file_paths <- file.path(py$temp_dir, zonal_temp_file_names)
# zonal_temp_file_paths <- file.path(temp_dir, zonal_temp_file_names)
all_data <- list()
for (file in zonal_temp_file_paths) {
  print(file)
  result <- tryCatch({
    read.csv(file)
  }, error = function(e) {
    warning(paste("Error reading file:", file, "- Skipping this file"))
    NULL
  })
  
  if (!is.null(result)) {
    if ("GNISIDNAME" %in% names(result) && !is.character(result$GNISIDNAME)) {
      result$GNISIDNAME <- as.character(result$GNISIDNAME)
    }
    all_data[[file]] <- result
  }
}

data_pre_z <- data_ci %>% dplyr::mutate(Julian_day = sprintf("%03d", as.numeric(Julian_day))) 
zonal_combined <- bind_rows(all_data) %>% 
  dplyr::left_join(resolvable_lakes, by=c("GNISIDNAME"="GNIS_Name_ID")) %>% 
  dplyr::mutate(File_NUM = as.character(File_NUM)) %>% 
  dplyr::left_join(dates, by=c("File_NUM"="CyAN_File_NUM")) %>% 
  dplyr::mutate(Month = lubridate::month(Date)) %>% 
  dplyr::mutate(Day = lubridate::day(Date)) %>% 
  dplyr::mutate(Date = as.Date(Date)) %>% 
  dplyr::distinct(Date,GNISIDNAME,.keep_all = TRUE) %>% 
  dplyr::select(GNISIDNAME,Area_Total_Cells,Date,MIN,MAX,MEAN,STD,MEDIAN,Year,Month,Day,Julian_day) %>% 
  dplyr::mutate(`7DDMGM` = as.numeric(NA),
                `7DADM` = as.numeric(NA),
                `Days of Data` = as.numeric(NA)) %>% 
  dplyr::bind_rows(data_pre_z[which(as.Date(data_pre_z$Date)>as.Date(max(sort(unique(data_pre_z$Date))))-14),]) %>% 
  dplyr::select(-c(`7DDMGM`,`7DADM`,`Days of Data`)) 


zonal_new <- data.frame()
for(gnis in sort(unique(zonal_combined$GNISIDNAME))) {
  # test: gnis = "Siltcoos Lake_01158483"
  print(gnis)
  
  dta_gnis <- zonal_combined %>% dplyr::filter(GNISIDNAME == gnis) %>% dplyr::arrange(Date) 
  
  dta_gnis_7d <- within(dta_gnis, {
    w <- seq_along(as.Date(dta_gnis$Date)) - findInterval(as.Date(dta_gnis$Date) - 7, as.Date(dta_gnis$Date))
    `7DADM` <- zoo::rollapplyr(MAX, w, mean)
    `7DDMGM` <- zoo::rollapply(MAX, w, FUN = function(x) exp(mean(log(x))), fill = NA, align = "right", partial = TRUE)
  })
  
  zonal_new <- rbind(zonal_new,dta_gnis_7d)
  
}

zonal <- zonal_new %>% 
  dplyr::mutate(Date = as.Date(Date)) %>% 
  dplyr::rename(`Days of Data` = w) %>% 
  dplyr::select(GNISIDNAME,Area_Total_Cells,Date,MIN,MAX,MEAN,STD,MEDIAN,Year,Month,Day,Julian_day,`7DDMGM`,`7DADM`,`Days of Data`) %>% 
  dplyr::arrange(Date,GNISIDNAME)  %>% 
  dplyr::filter(as.Date(Date) %in% as.Date(dates[which(dates$CyAN_File_NUM %in% download_file_list),]$Date))

zonal_update <- dplyr::bind_rows(data_ci,zonal) %>% dplyr::mutate(Date = as.Date(Date))
writexl::write_xlsx(zonal_update, paste0(data_path,"cyan_ci_2024.xlsx"))

```

```{r tabulate_tbl}
print("...Generate DN tabulate data table")
tabulate_temp_file_names <- paste0("tabulate_result_", download_file_list, ".csv")
tabulate_temp_file_paths <- file.path(py$temp_dir, tabulate_temp_file_names)

all_data <- list()
for (file in tabulate_temp_file_paths) {
  print(file)
  result <- tryCatch({
    read.csv(file)
  }, error = function(e) {
    warning(paste("Error reading file:", file, "- Skipping this file"))
    NULL
  })
  
  if (!is.null(result)) {
    if ("GNISIDNAME" %in% names(result) && !is.character(result$GNISIDNAME)) {
      result$GNISIDNAME <- as.character(result$GNISIDNAME)
    }
    all_data[[file]] <- result
  }
}

data_pre_t <- data_dn %>% dplyr::mutate(Julian_day = sprintf("%03d", as.numeric(Julian_day)))
tabulate_combined <- bind_rows(all_data) %>% 
  dplyr::left_join(dplyr::select(resolvable_lakes, GNIS_Name_ID, Area_Total_Cells), by=c("GNISIDNAME"="GNIS_Name_ID")) %>% 
  dplyr::mutate(File_NUM = as.character(File_NUM)) %>% 
  dplyr::left_join(dates, by=c("File_NUM"="CyAN_File_NUM")) %>% 
  dplyr::mutate(Date = as.Date(Date)) %>% 
  dplyr::select(-Day)
matched_columns <- dplyr::intersect(colnames(tabulate_combined), colnames(data_pre_t))
missing_columns <- dplyr::setdiff(colnames(data_pre_t), matched_columns)
library(rlang)
for (col in missing_columns) {
  tabulate_new <- tabulate_combined %>%
    dplyr::mutate(!!col := NA_real_)
}
cols <- colnames(tabulate_new)
first_cols <- c("GNISIDNAME","Area_Total_Cells","Date","Year","Julian_day","File_NUM")
num_part <- as.numeric(gsub("VALUE_", "", cols[grepl("^VALUE_", cols)]))
sorted_num_part <- sort(num_part)
new_cols <- c(first_cols, paste0("VALUE_", sorted_num_part))
tabulate <- tabulate_new[, new_cols]
tabulate_update <- dplyr::bind_rows(data_dn,tabulate) %>% dplyr::mutate(Date = as.Date(Date))
writexl::write_xlsx(tabulate_update, paste0(data_path,"cyan_dn_2024.xlsx"))

print("Step 2 completed")

```

## Step 3. Gether data for the shiny app.
```{r shiny_data}
# (1) Timeseries Data Table ----
dta1 <- resolvable_lakes

dta2 <- readxl::read_excel(paste0(data_path,"cyan_ci_2024.xlsx")) %>% 
  dplyr::filter(GNISIDNAME %in% dta1$inApp) # filter out saline lakes

dta3 <- readxl::read_excel(paste0(data_path,"cyan_ci_2002-2023.xlsx")) %>% 
  dplyr::filter(GNISIDNAME %in% dta1$inApp) # filter out saline lakes

dta <- rbind(dta2,dta3) %>% 
  dplyr::rename(`Daily Mean` = MEAN,
                `Daily Maximum` = MAX) %>% 
  tidyr::gather(`Summary Statistics`, `Cyanobacteria (cells/mL)`, 
                -GNISIDNAME,-Area_Total_Cells,-Date,-Year,-Month,-Day,-Julian_day,-`Days of Data`) %>% 
  tidyr::separate(GNISIDNAME,c("GNISNAME","GNISID"), sep="_") %>% 
  dplyr::mutate(GNISIDNAME = paste0(GNISNAME,"_",GNISID)) %>% 
  dplyr::mutate(Date = lubridate::ymd(Date)) %>% 
  dplyr::arrange(desc(Date)) %>% 
  dplyr::mutate(wi_DWSA = ifelse(GNISIDNAME %in% dta1$wi_DWSA, "Yes", "No")) %>% 
  dplyr::filter(Year > 2015)

# (2) Date Lookup Table ----
dates <- dates %>% dplyr::mutate(Date = lubridate::ymd(Date))

lookup.date <- dta %>% 
  dplyr::group_by(Date, Year, Day) %>% 
  dplyr::summarise(n=n()) %>% 
  dplyr::right_join(dates, by="Date") %>% 
  dplyr::rename(Year.dta = Year.x,
                Day.dta = Day.x,
                Year.dates = Year.y,
                JDay.dates = Day.y) %>% 
  dplyr::ungroup()

missing.dates <- lookup.date %>% 
  dplyr::filter(is.na(Day.dta))

# (3) Map: shapefiles ----
lakes.resolvable <- sf::st_read(dsn = paste0(data_path,"gis/CyAN_Waterbodies.shp"), layer = "CyAN_Waterbodies") %>% 
  sf::st_transform(crs = sf::st_crs("+init=EPSG:4326")) %>% sf::st_zm() %>% 
  dplyr::filter(GNISIDNAME %in% dta1$inApp) # filter out saline lakes

state.boundary <- sf::st_read(paste0(data_path,"gis/state_boundary_blm.shp")) %>% 
  sf::st_transform(crs = sf::st_crs("+init=EPSG:4326"))

huc6 <- sf::st_read(dsn = paste0(data_path,"gis/WBD_HU6.shp"), layer = "WBD_HU6") %>% 
  sf::st_transform(crs = sf::st_crs("+init=EPSG:4326"))

pal.huc6 <- leaflet::colorFactor(palette = c(RColorBrewer::brewer.pal(name="BrBG", n = 9), 
                                             RColorBrewer::brewer.pal(name="Paired", n = 9)), 
                                 domain = unique(sort(huc6$HU_6_NAME)))

# (4) Map: raster ----
# Raster color 
thevalues <- c(0,6310,20000,100000,7000000)

palette <- c('#bdbdbd','#66c2a4','#2ca25f','#006d2c')

pal.map <- leaflet::colorBin(palette = palette,
                             bins = c(0,6310,20000,100000,7000000),
                             domain = c(0,6310,20000,100000,7000000),
                             na.color = "transparent")
# Legend labels
labels = c("Non-detect","Low: 6,311 - 20,000","Moderate: 20,000 - 100,000","High: >100,000")

# (5) 7D Table  ----
dta_wide <- dta %>% 
  dplyr::filter((as.Date(Date) <= as.Date(max(dta2$Date))) & (as.Date(Date) >= as.Date(max(dta2$Date))-6)) %>% 
  tidyr::pivot_wider(names_from = `Summary Statistics`, values_from = `Cyanobacteria (cells/mL)`)

tbl.data <- dta_wide %>% 
  dplyr::group_by(GNISIDNAME) %>%
  dplyr::summarise(`7DDMGM` = max(`7DDMGM`)) %>%
  dplyr::inner_join(dta_wide, by = c("GNISIDNAME", "7DDMGM")) %>%
  dplyr::distinct(GNISIDNAME, .keep_all = TRUE) %>% 
  dplyr::left_join(lakes.resolvable, by = "GNISIDNAME") %>% 
  dplyr::mutate(Basin = ifelse(`HU_6_NAME` == "Willamette",`HU_8_NAME`,`HU_6_NAME`)) %>% 
  dplyr::select(GNISIDNAME,Basin,`7DDMGM`,`7DADM`,Date,`Days of Data`) %>% 
  dplyr::arrange(desc(`7DDMGM`)) %>% 
  dplyr::mutate(`7DDMGM` = ifelse(`7DDMGM`<= 6310, "Non-detect",format(round(`7DDMGM`,0),big.mark=",",scientific = FALSE))) %>%
  dplyr::mutate(`7DADM` = ifelse(`7DADM`<= 6310, "Non-detect",format(round(`7DADM`,0),big.mark=",",scientific = FALSE))) %>% 
  dplyr::rename(`Waterbody_GNISID*`= GNISIDNAME,
                `7DADM (cells/mL)` = `7DADM`,
                `7DDMGM (cells/mL)` = `7DDMGM`,
                `Date_7DDMGM` = Date)
  
map.tbl.data <- tbl.data %>% 
  dplyr::filter(!`7DDMGM (cells/mL)`== "Non-detect") %>% 
  dplyr::mutate(`7ddmgm` = gsub(",","",`7DDMGM (cells/mL)`)) %>%
  dplyr::filter(as.numeric(`7ddmgm`) > 100000) %>%
  dplyr::select(`Waterbody_GNISID*`,Basin,`7DDMGM (cells/mL)`,`7DADM (cells/mL)`,`Date_7DDMGM`,`Days of Data`,-`7ddmgm`)

# (6) 7D Map  ----
lakes.resolvable.7d <- lakes.resolvable %>% 
  dplyr::mutate(`7dadm` = ifelse(GNISIDNAME %in% map.tbl.data$`Waterbody_GNISID*`,"Waterbody with high cyanobacteria abundance","Others")) %>% 
  dplyr::filter(!`7dadm` == "Others")

palette7dadm <- leaflet::colorFactor(palette = c('#006d2c'), 
                                     domain = unique(sort(lakes.resolvable.7d$`7dadm`)))

palette7dadm_lg <- leaflet::colorFactor(palette = c('#00441b'), 
                                        domain = unique(sort(lakes.resolvable.7d$`7dadm`)))

tag.map.title <- htmltools::tags$style(htmltools::HTML("
  .leaflet-control.map-title { 
    transform: translate(-50%,20%);
    position: fixed !important;
    top: 5%;
    left: 10%;
    text-align: left;
    padding-left: 10px; 
    padding-right: 10px; 
    background: rgba(255,255,255,0.75);
    font-weight: bold;
    font-size: 28px;
  }
"))

map.title <- htmltools::tags$div(tag.map.title, htmltools::HTML("Highlighted Waterbodies during the Reporting Period"))

map.7d <- leaflet::leaflet() %>% 
  leaflet::addControl(map.title, position = "topleft", className="map-title") %>%
  leaflet::addProviderTiles("OpenStreetMap",group = "OpenStreetMap") %>% 
  leaflet::setView(lng = -120, lat = 44.4, zoom=7) %>% 
  leaflet::addPolygons(data = lakes.resolvable.7d, 
                       color = ~palette7dadm(lakes.resolvable.7d$`7dadm`),
                       weight = 2,
                       layer = ~lakes.resolvable.7d$GNISIDNAME,
                       smoothFactor = 0.5,
                       opacity = 1,
                       fillColor = "transparent",
                       fillOpacity = 0,
                       label = ~lakes.resolvable.7d$GNIS_Name,
                       labelOptions = leaflet::labelOptions(
                         noHide = TRUE,
                         textOnly = TRUE,
                         # opacity = 0.75,
                         direction = "right",
                         offset = c(15, 5),
                         style = list("font-size" = "16px","font-style" = "italic","color" = "blue")),
                       group = "lakes.resolvable.7d") %>% 
  # leaflegend::addLegendFactor(pal = palette7dadm_lg, 
  #                             values = lakes.resolvable.7d$`7dadm`,
  #                             shape = "rect",
  #                             opacity = 1,
  #                             fillOpacity = 0,
  #                             position = "topright") %>% 
  leaflet::addPolygons(data = state.boundary, 
                       color = "black",
                       weight = 2,
                       fillColor = "transparent",
                       fillOpacity = 1.0)

mapview::mapshot(map.7d, file = "./www/map_7d.jpg")

# (8) Boxplot Data Table  ----
# dn_tbl <- readxl::read_excel(paste0(data_path,"cyan_dn_2024.xlsx")) %>% 
#   dplyr::filter(GNISIDNAME %in% dta1$inApp) %>%  # filter out saline lakes
#   dplyr::filter(Date >= as.Date("2024-03-01")) %>% 
#   dplyr::select(-c(VALUE_254,VALUE_255)) %>% 
#   dplyr::mutate_at(vars(starts_with("VALUE_")), as.numeric) %>% 
#   dplyr::mutate_all(~replace(., is.na(.), 0)) %>% 
#   dplyr::mutate(total_counts = rowSums(across(c(VALUE_0:VALUE_253)))) %>% 
#   dplyr::filter(!total_counts == 0)

# Save data ----
save(#dn_tbl,
     dta,
     dta2,
     huc6,
     labels,
     lakes.resolvable,
     lookup.date,
     map.tbl.data,
     missing.dates,
     pal.huc6,
     pal.map,
     state.boundary,
     thevalues,
     file = "data.RData")

```
